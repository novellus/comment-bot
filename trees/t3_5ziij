{"controversiality": 0, "retrieved_on": 1427424895, "subreddit_id": "t5_2fwo", "created_utc": "1193836362", "ups": 2, "gilded": 0, "link_id": "t3_5ziij", "downs": 0, "score": 2, "body": "Someone has a bad case of unopened parens.", "id": "c02ccon", "author": "theeth", "author_flair_text": null, "distinguished": null, "edited": false, "archived": true, "subreddit": "programming", "name": "t1_c02ccon", "parent_id": "t3_5ziij", "score_hidden": false, "author_flair_css_class": null}
{"subreddit_id": "t5_2fwo", "gilded": 0, "downs": 0, "score": 2, "author": "filesalot", "created_utc": "1193837103", "children": [{"subreddit": "programming", "gilded": 0, "downs": 0, "score": 1, "author": "Felicia_Svilling", "created_utc": "1193839956", "children": [{"controversiality": 0, "retrieved_on": 1427424889, "subreddit_id": "t5_2fwo", "created_utc": "1193841077", "ups": 2, "gilded": 0, "link_id": "t3_5ziij", "downs": 0, "score": 2, "body": "Sorry! I meant _arbitrary_ precision, like Java BigDecimal.\n\nIn BigDecimal the programmer can control the _scale_ (how many decimal places) and rounding behavior.", "author": "filesalot", "author_flair_text": null, "distinguished": null, "edited": false, "author_flair_css_class": null, "archived": true, "subreddit": "programming", "name": "t1_c02cd6l", "parent_id": "t1_c02cd1w", "score_hidden": false, "id": "c02cd6l"}], "subreddit_id": "t5_2fwo", "ups": 1, "name": "t1_c02cd1w", "controversiality": 0, "author_flair_text": null, "link_id": "t3_5ziij", "retrieved_on": 1427424890, "score_hidden": false, "body": "That would be rather bad wouldn't it? I mean just computing 3.0/3.0 to infinite precision would fill up all avaible memory. (3.333333.. repeating forever)", "archived": true, "distinguished": null, "edited": false, "parent_id": "t1_c02ccr0", "id": "c02cd1w", "author_flair_css_class": null}, {"controversiality": 0, "retrieved_on": 1427424871, "subreddit_id": "t5_2fwo", "author_flair_text": null, "gilded": 0, "link_id": "t3_5ziij", "downs": 0, "score": 2, "score_hidden": false, "body": "The ECMAScript wiki says [128-bit IEEE 754r](http://www2.hursley.ibm.com/decimal/decbits.html), that is, max 34 digits of precision.", "author": "[deleted]", "created_utc": "1193850829", "distinguished": null, "edited": true, "archived": true, "id": "c02ceiw", "subreddit": "programming", "ups": 2, "parent_id": "t1_c02ccr0", "name": "t1_c02ceiw", "author_flair_css_class": null}], "subreddit": "programming", "name": "t1_c02ccr0", "score_hidden": false, "controversiality": 0, "archived": true, "ups": 2, "link_id": "t3_5ziij", "retrieved_on": 1427424895, "id": "c02ccr0", "author_flair_text": null, "distinguished": null, "edited": false, "parent_id": "t3_5ziij", "body": "Are decimals infinite precision, like bignums?", "author_flair_css_class": null}