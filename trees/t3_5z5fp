{"subreddit": "programming", "gilded": 0, "downs": 0, "score": -1, "author": "yters", "created_utc": "1193396340", "children": [{"subreddit_id": "t5_2fwo", "gilded": 0, "downs": 0, "score": 3, "author": "nglynn", "created_utc": "1193401249", "children": [{"subreddit_id": "t5_2fwo", "gilded": 0, "downs": 0, "score": 3, "author": "yters", "created_utc": "1193413352", "children": [{"subreddit": "programming", "gilded": 0, "downs": 0, "score": 2, "author": "nglynn", "created_utc": "1193415465", "children": [{"controversiality": 0, "downs": 0, "subreddit": "programming", "archived": true, "ups": 1, "gilded": 0, "link_id": "t3_5z5fp", "retrieved_on": 1427425240, "score": 1, "id": "c02bm2l", "author": "yters", "created_utc": "1193456418", "distinguished": null, "edited": false, "body": "Name me another way of possibly creating intelligence that is inherently non algorithmic.", "author_flair_text": null, "subreddit_id": "t5_2fwo", "name": "t1_c02bm2l", "parent_id": "t1_c02bhrv", "score_hidden": false, "author_flair_css_class": null}], "subreddit_id": "t5_2fwo", "name": "t1_c02bhrv", "score_hidden": false, "controversiality": 0, "archived": true, "ups": 2, "link_id": "t3_5z5fp", "retrieved_on": 1427425295, "id": "c02bhrv", "author_flair_text": null, "distinguished": null, "edited": false, "parent_id": "t1_c02bhir", "body": "Yes I noted that, but I personally fail to see why the incompleteness theorem would pose a problem. It doesn't seem to adversely effect our brains functioning so I fail to see how it's a problem for artificial systems of a similar nature.", "author_flair_css_class": null}], "subreddit": "programming", "score_hidden": false, "ups": 3, "controversiality": 0, "archived": true, "link_id": "t3_5z5fp", "retrieved_on": 1427425299, "id": "c02bhir", "author_flair_text": null, "distinguished": null, "edited": false, "name": "t1_c02bhir", "parent_id": "t1_c02bgay", "body": "If you read the paper, Godel himself thought it was likely.", "author_flair_css_class": null}, {"subreddit": "programming", "gilded": 0, "downs": 0, "score": 2, "author": "Homunculiheaded", "created_utc": "1193418973", "children": [{"subreddit_id": "t5_2fwo", "gilded": 0, "downs": 0, "score": 9, "author": "jerf", "created_utc": "1193420620", "children": [{"controversiality": 0, "downs": 0, "subreddit_id": "t5_2fwo", "author_flair_text": null, "name": "t1_c02bil7", "gilded": 0, "link_id": "t3_5z5fp", "retrieved_on": 1427425285, "score": 3, "score_hidden": false, "body": "Thanks for this clarification! \nAfter reading you comment I found that the wikipedia article on the [halting problem](http://en.wikipedia.org/wiki/Halting_problem#Can_humans_solve_the_halting_problem.3F) has a good section on this.", "author": "Homunculiheaded", "created_utc": "1193421438", "distinguished": null, "edited": false, "archived": true, "subreddit": "programming", "ups": 3, "parent_id": "t1_c02bih2", "id": "c02bil7", "author_flair_css_class": null}, {"controversiality": 0, "downs": 0, "subreddit": "programming", "created_utc": "1193421570", "ups": 3, "gilded": 0, "link_id": "t3_5z5fp", "retrieved_on": 1427425285, "score": 3, "score_hidden": false, "id": "c02bilz", "author": "lothair", "archived": true, "distinguished": null, "edited": false, "author_flair_text": null, "subreddit_id": "t5_2fwo", "name": "t1_c02bilz", "parent_id": "t1_c02bih2", "body": "As an example:\nAssume there is a function f(n).\nAlso assume the conjecture, that f(n)=0 for all natural numbers, is formally unprovable (or indpendent).\nNeither we, or any Turing machine, could tell whether a loop that checks this for every number and stops if f(n) =/= 0, would ever halt.", "author_flair_css_class": null}, {"subreddit": "programming", "gilded": 0, "downs": 0, "score": 2, "author": "tj9582", "created_utc": "1193422422", "children": [{"controversiality": 0, "downs": 0, "subreddit": "programming", "created_utc": "1193423644", "ups": 2, "gilded": 0, "link_id": "t3_5z5fp", "retrieved_on": 1427425281, "score": 2, "score_hidden": false, "body": " &gt; That's kind of cheating, though.\n\nI don't think you interpreted my point correctly. Human brains are particularly obviously weaker than some algorithms we can write, because we can't expand them.\n\nHowever, even if it was expandable, there's no reason to believe we could solve the halting problem. (Mathematically, anyhow. Maybe there's some way to trick the universe, or another universe, into doing infinite amounts of computation and giving us a useful result. But that's not what we mean by solving the halting problem, even though it might also do that.) \n\nIt's not the main point, it's a lawyer-esque \"Even if we concede prosecution point 1, prosecution's dependent point 2 would still be false.\"", "author": "jerf", "archived": true, "distinguished": null, "edited": true, "author_flair_text": null, "subreddit_id": "t5_2fwo", "name": "t1_c02biv8", "parent_id": "t1_c02bipf", "id": "c02biv8", "author_flair_css_class": null}], "subreddit_id": "t5_2fwo", "score_hidden": false, "ups": 2, "controversiality": 0, "author_flair_text": null, "link_id": "t3_5z5fp", "retrieved_on": 1427425283, "body": "That's kind of cheating, though. The halting problem isn't due to memory limitations. Even given infinite memory and time, a Turing machine can never solve it.\n\nSo the real question is, given infinite time and resources, is there some program for which the human race will never, ever be able to determine the haltibility of? Because unless there is, we can't be modeled as Turing machines, which has major ramifications across the board.", "archived": true, "distinguished": null, "edited": false, "name": "t1_c02bipf", "parent_id": "t1_c02bih2", "id": "c02bipf", "author_flair_css_class": null}], "subreddit": "programming", "name": "t1_c02bih2", "score_hidden": false, "controversiality": 0, "archived": true, "ups": 9, "link_id": "t3_5z5fp", "retrieved_on": 1427425286, "id": "c02bih2", "author_flair_text": null, "distinguished": null, "edited": true, "parent_id": "t1_c02bi7z", "body": "  &gt; You and me can deal with the halting problem, that's why work gets boring. You can look at a loop and relatively quickly tell whether or not it will end. Turing machines can't.\n\nThis is a common fallacy, but no, no you can't. You can look at trivial loops and apply some higher-level rules, almost all or all of which could also be fed to a prover, but you are no more capable of telling whether a loop will terminate in the _general_ case than anybody or anything else.\n\nYou can argue about whether there is something fundamentally interesting about the way we can tell such things that can not be reduced to a machine, but the claim that humans are walking halting-problem solvers is silly. The \"vast majority\" of loops (a tricky concept when talking about infinite sets, but forgive me) are too large for us to even _read_ in a single lifetime, let alone _solve_; a solution to the halting problem would be able to, given enough memory, but at the moment humans don't seem to have readily-available memory expansions. \n\nSolving the halting problem means you can do it on _all_ TMs, not a handful that our naive mechanical analysis chokes on but we happen to like.  ", "author_flair_css_class": null}], "subreddit_id": "t5_2fwo", "name": "t1_c02bi7z", "ups": 2, "controversiality": 0, "archived": true, "link_id": "t3_5z5fp", "retrieved_on": 1427425290, "score_hidden": false, "id": "c02bi7z", "author_flair_text": null, "distinguished": null, "edited": true, "parent_id": "t1_c02bgay", "body": "    I think this passage from the article gives the most insight:\n\n&gt;But in the light of G\u00f6del\u2019s incompleteness \ntheorem, there is an endless set of problems in elementary number theory for \nwhich such machines are inherently incapable of supplying answers, however \ncomplex their built-in mechanisms may be and however rapid their operations.  It \nmay very well be the case that the human brain is itself a machine with built-in \nlimitations of its own, and that there are mathematical problems which it is \nincapable of solving.  Even so, the human brain appears to embody a structure of \nrules of operation which is far more powerful than the structure of currently \nconceived artificial machines.\n\nThe issue is about whether Turing machines (or equivalent) are capable of emulating human-like thinking.  G\u00f6del\u2019s theorem points out that there are a great deal of unsolvable problems for turning machines, which the human mind (machine or no) can solve. \nHere's some examples:\nIf you are ever giving a Turing test, just ask whether a loop will terminate (halting problem). You and me can deal with the halting problem, that's why work gets boring. You can look at a loop and relatively quickly tell whether or not it will end.  Turing machines can't.\n\nAdditionally incompleteness says that all of mathematics cannot be derived from a single set of axioms.   \nIn this way even a brilliant Turing machine could not replicate the ability to come up with new sets of axioms and discover new branches of mathematics. The human mind is , quite remarkably, able to do this.  Even a brilliant TM could only operate from a given set of axioms, it would eventually exhaust the set of theorems and functions derived from those axioms but it would fail to discover additional branches of math on it's own.\n\nStrong AI may be possible, but not with a Turing Machine... at least that's my interpretation of things.\n    ", "author_flair_css_class": null}], "subreddit": "programming", "score_hidden": false, "ups": 3, "controversiality": 0, "archived": true, "link_id": "t3_5z5fp", "retrieved_on": 1427425315, "id": "c02bgay", "author_flair_text": null, "distinguished": null, "edited": false, "name": "t1_c02bgay", "parent_id": "t1_c02bg0y", "body": "Why would it?", "author_flair_css_class": null}, {"subreddit": "programming", "gilded": 0, "downs": 0, "score": 3, "author": "modulus", "created_utc": "1193412129", "children": [{"subreddit_id": "t5_2fwo", "gilded": 0, "downs": 0, "score": -4, "author": "yters", "created_utc": "1193413644", "children": [{"subreddit_id": "t5_2fwo", "gilded": 0, "downs": 0, "score": 4, "author": "nobodysbusiness", "created_utc": "1193413940", "children": [{"subreddit": "programming", "gilded": 0, "downs": 0, "score": 2, "author": "gwern", "created_utc": "1193420575", "children": [{"subreddit_id": "t5_2fwo", "gilded": 0, "downs": 0, "score": -1, "author": "yters", "created_utc": "1193458833", "children": [{"subreddit_id": "t5_2fwo", "gilded": 0, "downs": 0, "score": 3, "author": "lothair", "created_utc": "1193475862", "children": [{"subreddit": "programming", "gilded": 0, "downs": 0, "score": 0, "author": "yters", "created_utc": "1193530525", "children": [{"subreddit_id": "t5_2fwo", "gilded": 0, "downs": 0, "score": 2, "author": "redditcensoredme", "created_utc": "1193808145", "children": [{"subreddit_id": "t5_2fwo", "gilded": 0, "downs": 0, "score": 1, "author": "yters", "created_utc": "1193857821", "children": [{"controversiality": 0, "downs": 0, "subreddit": "programming", "archived": true, "ups": 2, "gilded": 0, "link_id": "t3_5z5fp", "retrieved_on": 1427424856, "score": 2, "body": "Yes you did. Do you want me to quote you?", "author": "redditcensoredme", "created_utc": "1193859372", "distinguished": null, "edited": false, "author_flair_text": null, "subreddit_id": "t5_2fwo", "score_hidden": false, "name": "t1_c02cfoq", "parent_id": "t1_c02cfi4", "id": "c02cfoq", "author_flair_css_class": null}], "subreddit": "programming", "name": "t1_c02cfi4", "score_hidden": false, "controversiality": 0, "archived": true, "ups": 1, "link_id": "t3_5z5fp", "retrieved_on": 1427424859, "id": "c02cfi4", "author_flair_text": null, "distinguished": null, "edited": false, "parent_id": "t1_c02cb9p", "body": "Nothing as far as I can tell, I didn't say it did.", "author_flair_css_class": null}], "subreddit": "programming", "score_hidden": false, "name": "t1_c02cb9p", "controversiality": 0, "author_flair_text": null, "ups": 2, "link_id": "t3_5z5fp", "retrieved_on": 1427424913, "body": "You mean philosophical consciousness, aka the nexus of subjective experience. And Lothair is precisely right. The independent instances do not have to be aware of each other. Assuming you accord ANY of them philosophical consciousness. Something I certainly don't accord to YOU since there is no proof whatsoever that you possess subjective experience.\r\n\r\nAnd really, what the fuck is Goedel's theorem supposed to have to do with this?", "archived": true, "distinguished": null, "edited": true, "parent_id": "t1_c02bq4b", "id": "c02cb9p", "author_flair_css_class": null}], "subreddit_id": "t5_2fwo", "name": "t1_c02bq4b", "score_hidden": false, "controversiality": 0, "archived": true, "ups": 0, "link_id": "t3_5z5fp", "retrieved_on": 1427425188, "id": "c02bq4b", "author_flair_text": null, "distinguished": null, "edited": true, "parent_id": "t1_c02bmvn", "body": "   Say strong AI were possible and I uploaded your mind into a computer to give you immortality.  Then you died and were never aware of being the mind uploaded on the computer, and just ceased being aware, period.  Would you say *you* were actually immortal?  Personally, I would consider myself scammed (though not really since I'd be dead).\n\nThere may be some semantic confusion going on here.  By consciousness I mean your awareness, the thing that is conscious of yourself and your surroundings, which is you.  ", "author_flair_css_class": null}], "subreddit": "programming", "score_hidden": false, "name": "t1_c02bmvn", "controversiality": 0, "archived": true, "ups": 3, "link_id": "t3_5z5fp", "retrieved_on": 1427425229, "id": "c02bmvn", "author_flair_text": null, "distinguished": null, "edited": false, "parent_id": "t1_c02bm76", "body": "How does it collide with the laws of physics, anyway?\n\nAnd that a consciousness would not \"be singular\" anymore isn't correct, since the independent instances of the consciousness don't have to be aware of each other.", "author_flair_css_class": null}, {"subreddit": "programming", "gilded": 0, "downs": 0, "score": 2, "author": "redditcensoredme", "created_utc": "1193807923", "children": [{"subreddit": "programming", "gilded": 0, "downs": 0, "score": 1, "author": "yters", "created_utc": "1193858481", "children": [{"controversiality": 0, "retrieved_on": 1427424856, "subreddit_id": "t5_2fwo", "archived": true, "gilded": 0, "link_id": "t3_5z5fp", "downs": 0, "score": 2, "score_hidden": false, "body": "&gt; being conscious of the fact that you are conscious (philosophical consciousness). \r\n\r\nWRONG. This is **not** philosophical consciousness. Philosophical consciousness is subjective experience. Period. That is what it is.\r\n\r\nWhat you are NOW describing is Dabrowski's multi-leveledness. Something which has no word to describe it in common language because it is extremely rare.\r\n\r\nYou really have very little understanding of what you are talking about.", "author": "redditcensoredme", "created_utc": "1193859785", "distinguished": null, "edited": true, "id": "c02cfqf", "author_flair_text": null, "subreddit": "programming", "ups": 2, "parent_id": "t1_c02cfkp", "name": "t1_c02cfqf", "author_flair_css_class": null}], "subreddit_id": "t5_2fwo", "ups": 1, "score_hidden": false, "controversiality": 0, "author_flair_text": null, "link_id": "t3_5z5fp", "retrieved_on": 1427424857, "body": "The highest form of human intelligence is being able to reflect upon and understand one's self.  This isn't just being conscious of other things (phsycological consciousness), but being conscious of the fact that you are conscious (philosophical consciousness).  Thus, intelligence itself conflates the two.  Any truely intelligent AI has to emulate this kind of intelligence, therefore must have philosophical consciousness.", "archived": true, "distinguished": null, "edited": false, "name": "t1_c02cfkp", "parent_id": "t1_c02cb9h", "id": "c02cfkp", "author_flair_css_class": null}], "subreddit_id": "t5_2fwo", "ups": 2, "name": "t1_c02cb9h", "controversiality": 0, "author_flair_text": null, "link_id": "t3_5z5fp", "retrieved_on": 1427424913, "score_hidden": false, "body": "You are conflating two very different definitions of consciousness. You are conflating psychological consciousness and philosophical consciousness.\r\n\r\nStrong AI says nothing about philosophical consciousness, only psychological consciousness. The multiplicity of psychological consciousness is completely irrelevant.\r\n\r\nWhether or not an AI is philosophically conscious is completely irrelevant. YOU do not possess philosophical consciousness because only **I** possess such. You are merely a zombie.\r\n\r\nIt is convenient for me to PRETEND that you are possessed of philosophical consciousness. It matters for such things as human rights and social contracts, all of which affect me.\r\n\r\nBut don't think for a moment that you are anything but a zombie to me. And a poor one at that. So you see, an AI zombie is no different from yourself.", "archived": true, "distinguished": null, "edited": true, "parent_id": "t1_c02bm76", "id": "c02cb9h", "author_flair_css_class": null}], "subreddit": "programming", "ups": -1, "score_hidden": false, "controversiality": 0, "author_flair_text": null, "link_id": "t3_5z5fp", "retrieved_on": 1427425238, "body": "Yes on your first sentence, no on the second.  Feeding the information corresponding to another location is similar to what we do anyways with TV and such, so that's not a problem.  Such a feed is not the same as being directly aware of a specific location.\n\nWhat I mean is that AI entails consciousness is reducible to information, or code.  Consequently, since code is the same no matter where it is run or how often it is copied, this means the exact same consciousness can be running in multiple simultaneous instances in geographically distinct locations.  Really, the mere notion of multiple simultaneous instances of a consciousness is incoherent, since I am by definition singular (not the same as multiple personalities) - I can't be a we.", "archived": true, "distinguished": null, "edited": false, "name": "t1_c02bm76", "parent_id": "t1_c02bigu", "id": "c02bm76", "author_flair_css_class": null}, {"controversiality": 0, "retrieved_on": 1427424913, "subreddit_id": "t5_2fwo", "archived": true, "name": "t1_c02cb91", "gilded": 0, "link_id": "t3_5z5fp", "downs": 0, "score": 1, "score_hidden": false, "id": "c02cb91", "author": "redditcensoredme", "created_utc": "1193807676", "distinguished": null, "edited": false, "author_flair_text": null, "subreddit": "programming", "ups": 1, "parent_id": "t1_c02bigu", "body": "You are an idiot who is willfully and deliberately confusing two completely different questions. That makes you unworthy of living.\r\n\r\nOh and Daniel Dennet is an idiot too.", "author_flair_css_class": null}], "subreddit_id": "t5_2fwo", "name": "t1_c02bigu", "ups": 2, "controversiality": 0, "author_flair_text": null, "link_id": "t3_5z5fp", "retrieved_on": 1427425286, "score_hidden": false, "body": "Because strong AI necessarily entails that there is nothing unique about the substrate, right, and that it is merely the patterns and processes said substrate can generate which generate intelligence/consciousness, right? So, if there's nothing unique about place, it should be possible to feed in to a mind running in one location the patterns corresponding to what it would perceive if it were running in another, and there seems to be nothing preventing us from feeding in a second set of patterns corresponding to a third location.\n\nAll these possible perplexities make it difficult to say for sure where you are at any time; if you're interested, Daniel Dennett has an entertaining paper on just this issue, appropriately titled \"Where Am I?\" (I think it's in his _Brainstorms_ anthology, but you can also find it in &lt;http://www.scribd.com/doc/6726/Hofstadter-Dennett-The-Minds-I&gt;).", "archived": true, "distinguished": null, "edited": false, "parent_id": "t1_c02bhlb", "id": "c02bigu", "author_flair_css_class": null}], "subreddit": "programming", "score_hidden": false, "ups": 4, "controversiality": 0, "author_flair_text": null, "link_id": "t3_5z5fp", "retrieved_on": 1427425299, "body": "&gt; Essentially, strong AI would entail I could be simultaneously conscious of multiple geographically distinct locations,\n\nI don't understand. Why would strong AI entail this?", "archived": true, "distinguished": null, "edited": false, "name": "t1_c02bhlb", "parent_id": "t1_c02bhjz", "id": "c02bhlb", "author_flair_css_class": null}], "subreddit": "programming", "score_hidden": false, "name": "t1_c02bhjz", "controversiality": 0, "author_flair_text": null, "ups": -4, "link_id": "t3_5z5fp", "retrieved_on": 1427425299, "body": "Yes, that's definitely what many people say, and is the safe, though boring, opinion.  \r\n\r\nHowever, in my opinion, besides the possibility of the incompleteness theorem disproving strong AI, there are just conceptual coherency problems with the notion of strong AI.  Essentially, strong AI would entail I could be simultaneously conscious of multiple geographically distinct locations, which doesn't jive with the laws of physics as I know them.", "archived": true, "distinguished": null, "edited": false, "parent_id": "t1_c02bhds", "id": "c02bhjz", "author_flair_css_class": null}], "subreddit_id": "t5_2fwo", "ups": 3, "score_hidden": false, "controversiality": 0, "author_flair_text": null, "link_id": "t3_5z5fp", "retrieved_on": 1427425301, "body": "Yeah, because no strong intelligence exists. er, wait, maybe it does... Maybe the human brain is an instance of a strong intelligence implemented on a physical substrate.\r\n", "archived": true, "distinguished": null, "edited": false, "name": "t1_c02bhds", "parent_id": "t1_c02bg0y", "id": "c02bhds", "author_flair_css_class": null}, {"subreddit": "programming", "gilded": 0, "downs": 0, "score": 3, "author": "asciilifeform", "created_utc": "1193417913", "children": [{"subreddit": "programming", "gilded": 0, "downs": 0, "score": -1, "author": "Homunculiheaded", "created_utc": "1193419876", "children": [{"subreddit": "programming", "gilded": 0, "downs": 0, "score": 3, "author": "asciilifeform", "created_utc": "1193420572", "children": [{"subreddit": "programming", "gilded": 0, "downs": 0, "score": 1, "author": "yters", "created_utc": "1193456003", "children": [{"subreddit_id": "t5_2fwo", "gilded": 0, "downs": 0, "score": 3, "author": "redditcensoredme", "created_utc": "1193807315", "children": [{"subreddit": "programming", "gilded": 0, "downs": 0, "score": 0, "author": "yters", "created_utc": "1193857970", "children": [{"controversiality": 0, "retrieved_on": 1427424856, "subreddit_id": "t5_2fwo", "created_utc": "1193859482", "ups": 1, "gilded": 0, "link_id": "t3_5z5fp", "downs": 0, "score": 1, "id": "c02cfp1", "author": "redditcensoredme", "archived": true, "distinguished": null, "edited": false, "author_flair_text": null, "subreddit": "programming", "score_hidden": false, "name": "t1_c02cfp1", "parent_id": "t1_c02cfil", "body": "AI systems have the same possibilities as human brains, the laws of physics are quite clear on this. This is the default option, the thing you must if you are a rational being believe, until proven otherwise.", "author_flair_css_class": null}], "subreddit_id": "t5_2fwo", "ups": 0, "name": "t1_c02cfil", "controversiality": 0, "author_flair_text": null, "link_id": "t3_5z5fp", "retrieved_on": 1427424859, "score_hidden": false, "body": "Who said anything about proving?  We're just talking about possibilities here, and I'm just clarifying the issue for ascii.", "archived": true, "distinguished": null, "edited": false, "parent_id": "t1_c02cb88", "id": "c02cfil", "author_flair_css_class": null}], "subreddit": "programming", "ups": 3, "name": "t1_c02cb88", "controversiality": 0, "archived": true, "link_id": "t3_5z5fp", "retrieved_on": 1427424914, "score_hidden": false, "id": "c02cb88", "author_flair_text": null, "distinguished": null, "edited": false, "parent_id": "t1_c02bm1r", "body": "Humunculi is an idiot and the \"system\" he describes is idiotic. And you are an idiot for giving it any credence whatsoever. Handwaving is not sufficient to prove the existence of an incomprehensible system. If you're going to be that much of an idiot, shut the fuck up.", "author_flair_css_class": null}], "subreddit_id": "t5_2fwo", "score_hidden": false, "name": "t1_c02bm1r", "controversiality": 0, "author_flair_text": null, "ups": 1, "link_id": "t3_5z5fp", "retrieved_on": 1427425240, "body": "Well, in that sense, we already build AI: every time we have kids.  But I doubt anyone means this when they talk about building AI.  We still assume it is possible to understand the AI system.  \n\nWith today's software systems, even if the builders don't understand the whole thing in reality, it is possible in theory.  This is different than what Homunculiheaded is describing, where we can't even understand the system in theory.", "archived": true, "distinguished": null, "edited": false, "parent_id": "t1_c02bigt", "id": "c02bm1r", "author_flair_css_class": null}], "subreddit_id": "t5_2fwo", "ups": 3, "name": "t1_c02bigt", "controversiality": 0, "author_flair_text": null, "link_id": "t3_5z5fp", "retrieved_on": 1427425287, "score_hidden": false, "body": "It is not necessary to fully understand something in order to build it. Most of today's software and other complex systems is proof of this.", "archived": true, "distinguished": null, "edited": false, "parent_id": "t1_c02bicx", "id": "c02bigt", "author_flair_css_class": null}, {"controversiality": 0, "downs": 0, "subreddit": "programming", "created_utc": "1193807445", "ups": 3, "gilded": 0, "link_id": "t3_5z5fp", "retrieved_on": 1427424914, "score": 3, "score_hidden": false, "body": "You are a fucking retard. The Halting problem has NOTHING whatsoever to do with \"understanding\". A better analogy is emulation. And oh lookee here, every Turing machine, and most computers, can virtualize themselves. Your \"case\" is made up of handwaving and hot air.", "author": "redditcensoredme", "author_flair_text": null, "distinguished": null, "edited": false, "archived": true, "subreddit_id": "t5_2fwo", "name": "t1_c02cb8k", "parent_id": "t1_c02bicx", "id": "c02cb8k", "author_flair_css_class": null}], "subreddit_id": "t5_2fwo", "score_hidden": false, "ups": -1, "controversiality": 0, "author_flair_text": null, "link_id": "t3_5z5fp", "retrieved_on": 1427425288, "body": "There is a pretty big caveat though: Strong AI could be very well impossible for *humans* to reproduce.  If there is to be Strong AI, I doubt very much that it will be done on a Turing machine, but that doesn\u2019t exclude the possibility that there is another system which is capable of this (which I believe is the point you are making, that the human mind exists and is itself proof that a machine for human like intelligence is possible).  \n\nHowever, Turing machines suck at understanding Turing machines, that\u2019s why we have the halting problem.  But suppose the Human mind, like a Turing machine is actually unable to completely understand its own computational processes.  In this way emulating human thinking may require the understanding of systems that are literally impossible for the human mind to understand.  \n\nI actually think there is a case to argue that most systems cannot describe themselves in full, they can only describe systems with which they have access to.  Wittgenstein made a similar argument regarding the fact that life after dead in the traditional sense would not answer any of the questions of life since in order to fully understand life you would have to be fundamentally outside of the system of being in the standard sense.\n", "archived": true, "distinguished": null, "edited": false, "name": "t1_c02bicx", "parent_id": "t1_c02bi36", "id": "c02bicx", "author_flair_css_class": null}, {"controversiality": 0, "downs": 0, "subreddit": "programming", "created_utc": "1193456317", "name": "t1_c02bm2f", "gilded": 0, "link_id": "t3_5z5fp", "retrieved_on": 1427425240, "score": 0, "score_hidden": false, "id": "c02bm2f", "author": "yters", "author_flair_text": null, "distinguished": null, "edited": false, "archived": true, "subreddit_id": "t5_2fwo", "ups": 0, "parent_id": "t1_c02bi36", "body": "An ironic response to a question of logic...", "author_flair_css_class": null}], "subreddit_id": "t5_2fwo", "name": "t1_c02bi36", "ups": 3, "controversiality": 0, "archived": true, "link_id": "t3_5z5fp", "retrieved_on": 1427425291, "score_hidden": false, "id": "c02bi36", "author_flair_text": null, "distinguished": null, "edited": true, "parent_id": "t1_c02bg0y", "body": " The **only** way for Strong AI to be impossible is if the human brain is powered by unique, irreproducible magic pixie dust. And if you believe that, give up doing science - you are worse than a creationist. ", "author_flair_css_class": null}], "subreddit_id": "t5_2fwo", "ups": -1, "name": "t1_c02bg0y", "controversiality": 0, "author_flair_text": null, "link_id": "t3_5z5fp", "retrieved_on": 1427425318, "score_hidden": false, "body": "Good find, I've been trying to figure out whether Godel's theorem logically disproves the possibility of strong AI.", "archived": true, "distinguished": null, "edited": false, "parent_id": "t3_5z5fp", "id": "c02bg0y", "author_flair_css_class": null}
{"subreddit": "programming", "gilded": 0, "downs": 0, "score": 7, "author": "lothair", "created_utc": "1193398891", "children": [{"subreddit": "programming", "gilded": 0, "downs": 0, "score": 5, "author": "DannoHung", "created_utc": "1193407806", "children": [{"controversiality": 0, "downs": 0, "subreddit": "programming", "created_utc": "1193416691", "name": "t1_c02bhxb", "gilded": 0, "link_id": "t3_5z5fp", "retrieved_on": 1427425294, "score": 6, "score_hidden": false, "id": "c02bhxb", "author": "kitanokikori", "author_flair_text": null, "distinguished": null, "edited": false, "archived": true, "subreddit_id": "t5_2fwo", "ups": 6, "parent_id": "t1_c02bgve", "body": "If you're too lazy to put the \u00f6, it's also acceptable to type 'oe' =&gt; \"Goedel\"", "author_flair_css_class": null}], "subreddit_id": "t5_2fwo", "ups": 5, "name": "t1_c02bgve", "controversiality": 0, "author_flair_text": null, "link_id": "t3_5z5fp", "retrieved_on": 1427425307, "score_hidden": false, "body": "My keyboard isn't metal enough...", "archived": true, "distinguished": null, "edited": false, "parent_id": "t1_c02bg5i", "id": "c02bgve", "author_flair_css_class": null}], "subreddit_id": "t5_2fwo", "name": "t1_c02bg5i", "ups": 7, "controversiality": 0, "archived": true, "link_id": "t3_5z5fp", "retrieved_on": 1427425319, "score_hidden": false, "id": "c02bg5i", "author_flair_text": null, "distinguished": null, "edited": false, "parent_id": "t3_5z5fp", "body": "It's G\u00f6del not Godel, you are causing eye cancer!", "author_flair_css_class": null}
{"controversiality": 0, "retrieved_on": 1427425306, "subreddit_id": "t5_2fwo", "created_utc": "1193409481", "ups": 1, "gilded": 0, "link_id": "t3_5z5fp", "downs": 0, "score": 1, "id": "c02bh1w", "author": "Tommah", "archived": true, "distinguished": null, "edited": false, "body": "Like Spy vs. Spy, but with fewer bombs and more prime numbers.", "author_flair_text": null, "subreddit": "programming", "name": "t1_c02bh1w", "parent_id": "t3_5z5fp", "score_hidden": false, "author_flair_css_class": null}