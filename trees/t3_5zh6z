
{"controversiality": 0, "downs": 0, "subreddit": "programming", "ups": 4, "author_flair_text": null, "name": "t1_c02cadn", "gilded": 0, "link_id": "t3_5zh6z", "retrieved_on": 1427424925, "score": 4, "id": "c02cadn", "author": "cypherx", "created_utc": "1193796640", "distinguished": null, "edited": false, "archived": true, "subreddit_id": "t5_2fwo", "score_hidden": false, "parent_id": "t3_5zh6z", "body": "1) \"These are obtained by training the network with known input data (see Resources), and no real analysis will glean objective meaning from these numbers; they are truly magic. \"\nFunny, I thought the weights represented a high dimensional hyperplane, which (when projected onto the input space) creates a non-linear boundary between input categories.   I guess thinking of them as opaque magic is more useful.\n\n2) Where is the transform function? I've never seen a neural network that doesn't threshold or squash the weighted sum of each neuron.\n\n3) Where is the training algorithm? Forward propagation alone is useless.\n\n4) Can any amount of parallel scale overcome the huge overhead of text encoding/decoding and network transport? I'm skeptical that even the entire internet (with one neuron per computer transmitting outputs in text files) could outperform a multilayer perceptron running on my laptop (reasonably coded in matlab). ", "author_flair_css_class": null}