{"subreddit_id": "t5_2fwo", "gilded": 0, "downs": 0, "score": 0, "author": "y_gingras", "created_utc": "1192945273", "children": [{"subreddit": "programming", "gilded": 0, "downs": 0, "score": 7, "author": "bad_code", "created_utc": "1192988360", "children": [{"controversiality": 0, "retrieved_on": 1427425838, "subreddit_id": "t5_2fwo", "archived": true, "name": "t1_c02ahf5", "gilded": 0, "link_id": "t3_5yrj4", "downs": 0, "score": 1, "score_hidden": false, "body": "Yeah, that was pretty surprising.  Cross-validation stats are pretty standard for this sort of test scenario.", "author": "Mr_Smartypants", "created_utc": "1193018767", "distinguished": null, "edited": false, "author_flair_text": null, "subreddit": "programming", "ups": 1, "parent_id": "t1_c02aeyy", "id": "c02ahf5", "author_flair_css_class": null}], "subreddit_id": "t5_2fwo", "name": "t1_c02aeyy", "score_hidden": false, "controversiality": 0, "author_flair_text": null, "ups": 7, "link_id": "t3_5yrj4", "retrieved_on": 1427425869, "id": "c02aeyy", "archived": true, "distinguished": null, "edited": false, "parent_id": "t1_c02acx9", "body": "Classifying text into one of two groups (spam/not-spam) is something a **random** classifier can do with 50% accuracy.  They tried classifying documents into 10 to 20 classes leaving much more room for error.  Their classification step separated the classes by *author*, a much subtler classifier than spam/not-spam considering pre-generated spam has a higher occurrence of non-grammatical statements and spelling errors.  Also, they don't just say they do better than a Naive Bayes approach, they actually *use* a Naive Bayes approach (augmented by n-gram analysis).  \n\nIn fact, thats one of the three problems with this horrible paper.  First, it is a known result that n-gram based classification works better than standard NB, because a standard NB assumes independence from word to word in a text, which is a very poor assumption.  When you widen your scope to include N words of context you increase the probability that your classifier is picking up grammatical and syntactical knowledge about the language.  The drawback is that they increase memory requirements as a factor of the size of the n-gram. Second, their experimental design is shit. Their testing size compared to the available documents was absurdly small.  They said nothing about repeated experimentation  They did not report variance statistics.  They did not provide the variance statistics from the cited results of other works.  This paper is crap, but not because of the arguments you make.", "author_flair_css_class": null}], "subreddit": "programming", "name": "t1_c02acx9", "score_hidden": false, "controversiality": 0, "archived": true, "ups": 0, "link_id": "t3_5yrj4", "retrieved_on": 1427425896, "id": "c02acx9", "author_flair_text": null, "distinguished": null, "edited": true, "parent_id": "t3_5yrj4", "body": " They tested it by putting usenet posts in the right news group.  Fine but that's not what we typically use a classifier for.  If you are going to claim that you do better than naive bayasian, the very least would be to show how well you do on the [Spam Assassin corpus](http://spamassassin.apache.org/publiccorpus/).", "author_flair_css_class": null}
{"controversiality": 0, "downs": 0, "subreddit": "programming", "author_flair_text": null, "name": "t1_c02adbs", "gilded": 0, "link_id": "t3_5yrj4", "retrieved_on": 1427425891, "score": 3, "score_hidden": false, "id": "c02adbs", "author": "jjmac", "created_utc": "1192952518", "distinguished": null, "edited": false, "archived": true, "subreddit_id": "t5_2fwo", "ups": 3, "parent_id": "t3_5yrj4", "body": "You can do this already with SQL Server Data Mining.  This tutorial (http://www.sqlserverdatamining.com/DMCommunity/Tutorials/Links_LinkRedirector.aspx?id=689) (free login required) shows how, but so you don't have to go into it (and it doesn't talk about the underlying technology anyway), the text mining components of Integration Services use markov chain approaches to model grammar to extract noun and noun phrases and then you can use the NB (or Logistic Regression, or Trees, or Association Rules) of the Data Mining in SQL Server Analysis Services to mine the data to detect patterns.  Works great for classifying web feedback into a taxonomy, for example - which is much harder than usenet posts, since they are generally shorter with less context.", "author_flair_css_class": null}
{"subreddit_id": "t5_2fwo", "gilded": 0, "downs": 0, "score": -6, "author": "revonrat", "created_utc": "1192956533", "children": [{"controversiality": 0, "downs": 0, "subreddit": "programming", "archived": true, "ups": 8, "gilded": 0, "link_id": "t3_5yrj4", "retrieved_on": 1427425886, "score": 8, "body": "reddit is not all about news. It's has also links to intresting things you did not know. ", "author": "[deleted]", "created_utc": "1192963744", "distinguished": null, "edited": false, "id": "c02adpu", "author_flair_text": null, "subreddit_id": "t5_2fwo", "name": "t1_c02adpu", "parent_id": "t1_c02adhm", "score_hidden": false, "author_flair_css_class": null}, {"controversiality": 0, "retrieved_on": 1427425852, "subreddit_id": "t5_2fwo", "archived": true, "name": "t1_c02agd1", "gilded": 0, "link_id": "t3_5yrj4", "downs": 0, "score": 1, "score_hidden": false, "id": "c02agd1", "author": "sjf", "created_utc": "1193005983", "distinguished": null, "edited": false, "author_flair_text": null, "subreddit": "programming", "ups": 1, "parent_id": "t1_c02adhm", "body": "Because all those blog posts reiterating the same thing sure doesn't sound like news to me.", "author_flair_css_class": null}, {"controversiality": 0, "retrieved_on": 1427425839, "subreddit_id": "t5_2fwo", "created_utc": "1193017704", "ups": 1, "gilded": 0, "link_id": "t3_5yrj4", "downs": 0, "score": 1, "body": "\"Homer is new this morning, and perhaps nothing is as old as today's newspaper.\"\n\n--- [Charles P\u00e9guy](http://en.wikipedia.org/wiki/Charles_P\u00e9guy)", "author": "Tommah", "archived": true, "distinguished": null, "edited": false, "id": "c02ahc4", "author_flair_text": null, "subreddit": "programming", "name": "t1_c02ahc4", "parent_id": "t1_c02adhm", "score_hidden": false, "author_flair_css_class": null}], "subreddit": "programming", "score_hidden": false, "ups": -6, "controversiality": 0, "author_flair_text": null, "link_id": "t3_5yrj4", "retrieved_on": 1427425889, "body": "So, now we're posting 4-year old papers as news?", "archived": true, "distinguished": null, "edited": false, "name": "t1_c02adhm", "parent_id": "t3_5yrj4", "id": "c02adhm", "author_flair_css_class": null}
{"controversiality": 0, "downs": 0, "subreddit_id": "t5_2fwo", "created_utc": "1192982099", "name": "t1_c02aeho", "gilded": 0, "link_id": "t3_5yrj4", "retrieved_on": 1427425877, "score": 3, "score_hidden": false, "body": "Possible application: Automatically read reddit submissions and decide which subreddit they belong in.", "author": "[deleted]", "archived": true, "distinguished": null, "edited": false, "author_flair_text": null, "subreddit": "programming", "ups": 3, "parent_id": "t3_5yrj4", "id": "c02aeho", "author_flair_css_class": null}